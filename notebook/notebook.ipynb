{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cargar el Dataset y el módelo de lenguaje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/diego/UValencia/DataMining/project/dataset/mLabel_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Primero cargo un modelo en spacy, libreria de procesado de texto\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesado del texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "def normalizar_tweet(doc):\n",
    "    '''Función que normaliza un texto cogiendo sólo\n",
    "    las palabras en minúsculas mayores de 3 caracteres'''\n",
    "    # separamos en tokens\n",
    "    tokens = nlp(doc)\n",
    "    # filtramos stopwords\n",
    "    filtered_tokens = [t.lower_ for t in tokens if\n",
    "                       len(t.text)>3 and\n",
    "                       not t.is_punct and\n",
    "                       not t.lower_.startswith(\"@\") and\n",
    "                       not t.lower_.startswith(\"#\") and\n",
    "                       not t.lower_.startswith(\"http\") and\n",
    "                       not t.lower_ in stop_words]\n",
    "    # juntamos de nuevo en una cadena\n",
    "    doc = ' '.join(filtered_tokens)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"@cath__kath AstraZeneca is made with the kidney cells of a little girl aborted back in the 70s\"\n",
    "normalizar_tweet(string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tweet_normalizado'] = df['tweet'].apply(normalizar_tweet)\n",
    "df['labels_list'] = df['labels'].apply(lambda x: x.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df.labels_list.value_counts():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparamos los labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "binaryEncoder = MultiLabelBinarizer()\n",
    "binary_labels = binaryEncoder.fit_transform(df['labels_list'])\n",
    "binary_labels.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis del Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = df['labels_list'].value_counts()\n",
    "\n",
    "print(frequency[:30])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrices TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.tweet_normalizado, binary_labels, test_size=0.2, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Ajustar y transformar los textos de tweets a una matriz TF-IDF\n",
    "tfidf_matrix = tfidf_vectorizer.fit(df['tweet_normalizado'])\n",
    "\n",
    "x_train_tfidf = tfidf_matrix.transform(X_train)\n",
    "x_test_tfidf = tfidf_matrix.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model_id = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "hf_token = \"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\"\n",
    "import requests\n",
    "\n",
    "api_url = f\"https://api-inference.huggingface.co/pipeline/feature-extraction/{model_id}\"\n",
    "headers = {\"Authorization\": f\"Bearer {hf_token}\"}\n",
    "def query(texts):\n",
    "    response = requests.post(api_url, headers=headers, json={\"inputs\": texts, \"options\":{\"wait_for_model\":True}})\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_embeddings = query(list(X_train))\n",
    "\n",
    "x_test_embeddings = query(list(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_embeddings = query(list(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3426    question taken post blood test fact immune cel...\n",
       "8452    funny shit comes cold hell marty makary neck n...\n",
       "6854    people died taking pfizer vaccine according pf...\n",
       "2119    bioedge governments sweeten vaccine pill incen...\n",
       "6534    wants shitty vaccine research chinese vaccines...\n",
       "                              ...                        \n",
       "3960    astrazeneca manufacturing epic failure edsel l...\n",
       "4853    answer lies result happens want vaccine vaccin...\n",
       "887            okay pfizer women infertile want hell want\n",
       "3854    telling sask people mounting deaths visious af...\n",
       "3556    people choice untried gauranteed work safe vac...\n",
       "Name: tweet_normalizado, Length: 1985, dtype: object"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entreno Módelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "for i in range(5,6):\n",
    "    knn = OneVsRestClassifier(KNeighborsClassifier(i)).fit(x_train_embeddings,y_train)\n",
    "    y_pred = knn.predict(x_test_embeddings)\n",
    "    print_report(y_test,y_pred,f'{i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "\n",
    "\n",
    "# Define la arquitectura de la red neuronal\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(x_train_embeddings[0])+1, output_dim=64, input_length=384))\n",
    "model.add(SimpleRNN(24, return_sequences=True))\n",
    "model.add(SimpleRNN(24))\n",
    "model.add(Dense(12, activation='sigmoid')) # Using sigmoid for multilabel classification\n",
    "\n",
    "\n",
    "# Compila el modelo\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['categorical_accuracy'])\n",
    "# Entrena el modelo\n",
    "print(model.summary)\n",
    "model.fit(np.array(x_train_embeddings), np.array(y_train), epochs=5, batch_size=32, validation_data=(np.array(x_test_embeddings), np.array(y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles, make_classification, make_moons\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "names = [\n",
    "    \"Nearest Neighbors\",\n",
    "    \"Linear SVM\",\n",
    "    \"RBF SVM\",\n",
    "\n",
    "    \"Decision Tree\",\n",
    "\n",
    "    \"Neural Net\",\n",
    "    \"AdaBoost\",\n",
    "    \"Naive Bayes\",\n",
    "    \"QDA\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def print_report(y_test, y_pred,name):\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    with open('classification_report.txt', 'a') as f:\n",
    "        f.write(f\"////////////////////{name}/////////////////////\\n\")\n",
    "        f.write(report)\n",
    "\n",
    "classifiers = [\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "\n",
    "    KNeighborsClassifier(5),\n",
    "    SVC(kernel=\"linear\", C=0.025, random_state=42),\n",
    "    SVC(gamma=2, C=1, random_state=42),\n",
    "\n",
    "    DecisionTreeClassifier(),\n",
    "\n",
    "    MLPClassifier(),\n",
    "    AdaBoostClassifier(algorithm=\"SAMME\", random_state=42),\n",
    "\n",
    "]\n",
    "\n",
    "for clf,name in zip(classifiers,names):\n",
    "    clf = MultiOutputClassifier(clf)\n",
    "    clf.fit(np.array(x_train_embeddings),np.array(y_train))\n",
    "    y_pred = clf.predict(np.array(x_test_embeddings))\n",
    "    print_report(np.array(y_test),np.array(y_pred),name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Red Neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define la arquitectura de la red neuronal\n",
    "model = keras.Sequential([\n",
    "\n",
    "    Dropout(0.3),\n",
    "    keras.layers.Dense(12, activation='relu'),\n",
    "    Dropout(0.3),\n",
    "    keras.layers.Dense(12, activation='sigmoid')  # Capa de salida con activación sigmoide\n",
    "])\n",
    "\n",
    "# Compila el modelo\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['categorical_accuracy'])\n",
    "model.summary\n",
    "# Entrena el modelo\n",
    "\n",
    "model.fit(np.array(x_train_embeddings), np.array(y_train), epochs=50, batch_size=32, validation_data=(np.array(x_test_embeddings), np.array(y_test)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(np.array(x_test_embeddings))\n",
    "def threshold_array(array, threshold):\n",
    "    return np.where(array > threshold, 1, 0)\n",
    "\n",
    "y_pred_normaliced = list(map(lambda array: threshold_array(array, 0.3), y_pred))\n",
    "print(classification_report(y_test, y_pred_normaliced))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def print_report(y_test, y_pred,name):\n",
    "    report = classification_report(y_test, y_pred)\n",
    "    with open('classification_report.txt', 'a') as f:\n",
    "        f.write(f\"////////////////////{name}/////////////////////\")\n",
    "        f.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for cm in multilabel_confusion_matrix(y_test,y_pred_normaliced):\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss\n",
    "print('Hamming Loss: ', round(hamming_loss(y_test, y_pred_normaliced),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[10:15],binary_labels[10:15]\n",
    "def entreno(i):\n",
    "    print(model.predict(x_test_embeddings[i]))\n",
    "    print(y_test[i])\n",
    "    print()\n",
    "for i in range(5):\n",
    "    entreno(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
